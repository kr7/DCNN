{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjk+DN8awagls/4mth7/5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kr7/DCNN/blob/main/DCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFDH9eUsb63l"
      },
      "source": [
        "**Dynamic Convolutional Neural Networks for Time Series Classification**\n",
        "\n",
        "This notebook illustrates how to use dynamic convolutional neural networks for time series classification and it allows to reproduce our experiments reported in:\n",
        "\n",
        "K. Buza, M. Antal (2021): *Covolutional neural networks with dynamic convolution for time series classification*\n",
        "\n",
        "In order to run this notebook, you need an approriately configured IPython notebook server. For simplicity, you can (try to) run this notebook in Google Colab (https://colab.research.google.com), it should work with Google Colab :-)\n",
        "\n",
        "In order to fully understand our model you are welcome to read the aforementioned manuscript. If you have any further questions, please feel free to contact us at buza@biointelligence.hu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWr4AIRyeIZN"
      },
      "source": [
        "**1. Import libraries** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeN7nZk6eCFx"
      },
      "source": [
        "import numpy as np \n",
        "import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from numpy import genfromtxt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7W4y1wHcy1-"
      },
      "source": [
        "**2. Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w05p3af9F1h2"
      },
      "source": [
        "# Download the data\n",
        "!wget http://www.timeseriesclassification.com/Downloads/Archives/Univariate2018_arff.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTL3F3XkF-GX"
      },
      "source": [
        "# Extract the archive\n",
        "!unzip Univariate2018_arff.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxCUmYuOd7AE"
      },
      "source": [
        "**If you want to run the experiment on other datasets**, please change *file_name_prefix* here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T0P-P11ehT2"
      },
      "source": [
        "file_name_prefix = \"Univariate_arff/Adiac/Adiac\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1OKFZLqlED-"
      },
      "source": [
        "# Load train and test data\n",
        "\n",
        "train_data_with_labels = np.genfromtxt(file_name_prefix+'_TRAIN.txt')\n",
        "test_data_with_labels = np.genfromtxt(file_name_prefix+'_TEST.txt')\n",
        "\n",
        "data_with_labels = np.vstack( (train_data_with_labels, test_data_with_labels))\n",
        "data = data_with_labels[:,1:]\n",
        "labels = data_with_labels[:,0]\n",
        "\n",
        "\n",
        "# We make sure that labels are numbered as 0, 1, 2, ... \n",
        "# and set the number of classes\n",
        "\n",
        "min_label = min(labels)\n",
        "max_label = max(labels)\n",
        "if min_label == 0:\n",
        "  NUM_CLASSES = int(max_label+1)\n",
        "elif min_label == 1:\n",
        "  labels = labels - min_label\n",
        "  NUM_CLASSES = int(max_label)\n",
        "elif min_label == -1:\n",
        "  if np.sum(labels == -1)+np.sum(labels==1) == len(labels):\n",
        "    NUM_CLASSES = 2\n",
        "    labels[labels==-1]=0\n",
        "  else:\n",
        "    raise Exception(\"Unexpected labels\")\n",
        "else:\n",
        "  raise Exception(\"Unexpected labels\")\n",
        "\n",
        "# We make sure that the length of the time series is a multiple of 4 \n",
        "# (for Net1, we acutally only need the length to be a multiple of 2,\n",
        "# but we need the length to be a multiple of 4 for Net2)\n",
        "\n",
        "NUM_INPUT_FEATURES = len(data[0]) \n",
        "values_to_cut = NUM_INPUT_FEATURES % 4\n",
        "if values_to_cut != 0:\n",
        "  data = data[:,0:NUM_INPUT_FEATURES-values_to_cut]\n",
        "  NUM_INPUT_FEATURES = NUM_INPUT_FEATURES - values_to_cut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB0NvMREiJI6"
      },
      "source": [
        "**3. Functions for DTW calculations**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8RkDNR7ipui"
      },
      "source": [
        "We use the following function for the calculation of dynamic time warping (DTw) distances. In order to allow for quick DTW calculations, the function is implemented in Cython."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVj233U4kac-"
      },
      "source": [
        "%load_ext cython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToF2ZCY2o7OA"
      },
      "source": [
        "%%cython\n",
        "\n",
        "import numpy as np\n",
        "cimport numpy as np\n",
        "\n",
        "def dtw(np.ndarray[np.float_t, ndim=1] ts1, \n",
        "        np.ndarray[np.float_t, ndim=1] ts2):\n",
        "  \n",
        "  cdef int LEN_TS1 \n",
        "  cdef int LEN_TS2\n",
        "  cdef int i\n",
        "  cdef int j\n",
        "  cdef np.ndarray[np.float_t, ndim=2] dtw_matrix\n",
        "\n",
        "  LEN_TS1 = len(ts1)\n",
        "  LEN_TS2 = len(ts2)\n",
        "\n",
        "  dtw_matrix = np.zeros( (LEN_TS1, LEN_TS2), dtype=np.float )\n",
        "  \n",
        "  dtw_matrix[0,0] = abs(ts1[0]-ts2[0])\n",
        "  \n",
        "  for i in range(1, LEN_TS1):\n",
        "    dtw_matrix[i,0] = dtw_matrix[i-1,0]+abs(ts1[i]-ts2[0])\n",
        "\n",
        "  for j in range(1, LEN_TS2):\n",
        "    dtw_matrix[0,j] = dtw_matrix[0,j-1]+abs(ts1[0]-ts2[j])\n",
        "\n",
        "  for i in range(1, LEN_TS1):\n",
        "    for j in range(1, LEN_TS2):\n",
        "      dtw_matrix[i,j] = min(dtw_matrix[i-1,j-1], dtw_matrix[i-1,j], \n",
        "                            dtw_matrix[i, j-1]) + abs(ts1[i]-ts2[j])\n",
        "  \n",
        "  return dtw_matrix[ len(ts1)-1, len(ts2)-1 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Uo7bV7fex7"
      },
      "source": [
        "We use the following function to calculate the activations of the dynamic convolutional layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS6dT3KhfcDp"
      },
      "source": [
        "def dc_activations(data, convolutional_filters):\n",
        "  \"\"\"\n",
        "  Calculation of the activations of the dynamic convolutional layer.\n",
        "\n",
        "  Inputs\n",
        "  ------\n",
        "    data : np.array \n",
        "      Two-dimensional array containing the input data, \n",
        "      each row of the array corresponds to one of the time series\n",
        "    convolutional_filters : np.array\n",
        "      Three-dimensional array containing the parameters of the dynamic \n",
        "      convolutional layer. The first index corresponds to the output channel\n",
        "      of the convolution, the second index corresponds to the input channel \n",
        "      (the current implementation only works with 1 input channel, so the second\n",
        "      index is always zero), the third index is the position within the local\n",
        "      pattern corresponding to a convolutional filter\n",
        "  \"\"\"\n",
        "  num_instances = len(data)\n",
        "  length_of_time_series = len(data[0])\n",
        "  num_conv_filters = len(convolutional_filters)\n",
        "  conv_filter_size = len(convolutional_filters[0][0])\n",
        "\n",
        "  activations = np.zeros( (num_instances, num_conv_filters, \n",
        "                           length_of_time_series-conv_filter_size+1) )\n",
        "  for i in range(num_instances):\n",
        "    for j in range(length_of_time_series-conv_filter_size+1):\n",
        "      for k in range(num_conv_filters):\n",
        "        activations[i,k,j] = dtw(convolutional_filters[k][0],\n",
        "                                 data[i,j:j+conv_filter_size])\n",
        "        \n",
        "  return activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GGw86l4irwx"
      },
      "source": [
        "**4. Definition of the neural networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVDeXWErBYlO"
      },
      "source": [
        "CONV_FILTERS = 25\n",
        "CONV_FILTERS2 = 10\n",
        "CONV_FILTER_SIZE = 9\n",
        "\n",
        "class Net1_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net1_CNN, self).__init__()\n",
        "        num_units_fc = 100\n",
        "        self.num_inputs_fc = int(CONV_FILTERS*(NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2)\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels = CONV_FILTERS, \n",
        "                               kernel_size=CONV_FILTER_SIZE, padding = 0, stride = 1)\n",
        "        self.max_pool = nn.MaxPool1d(2)\n",
        "        self.fc = nn.Linear(self.num_inputs_fc, num_units_fc)\n",
        "        self.out = nn.Linear(num_units_fc, NUM_CLASSES) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, NUM_INPUT_FEATURES)\n",
        "        x = self.conv1(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = x.view(-1, self.num_inputs_fc)\n",
        "        x = torch.relu(self.fc(x))\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Net2_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2_CNN, self).__init__()\n",
        "        num_units_fc = 100\n",
        "        self.num_inputs_fc = int(CONV_FILTERS2*((NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2-CONV_FILTER_SIZE+1)/2)\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels = CONV_FILTERS, \n",
        "                               kernel_size=CONV_FILTER_SIZE, padding = 0, stride = 1)\n",
        "        self.max_pool = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels = CONV_FILTERS, out_channels = CONV_FILTERS2, \n",
        "                               kernel_size=CONV_FILTER_SIZE, padding = 0, stride = 1)\n",
        "        self.max_pool2 = nn.MaxPool1d(2)\n",
        "        self.fc = nn.Linear(self.num_inputs_fc, num_units_fc)\n",
        "        self.out = nn.Linear(num_units_fc, NUM_CLASSES) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, NUM_INPUT_FEATURES)\n",
        "        x = self.conv1(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.max_pool2(x)\n",
        "        x = x.view(-1, self.num_inputs_fc)\n",
        "        x = torch.relu(self.fc(x))\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Please note that the dynamic convolutional layer is initialized using the \n",
        "# parameters learned during the \"pre-train\" phase (in which a \"usual\" \n",
        "# convolutional network is trained). Once the pre-train phased is completed, \n",
        "# the parameters of the dynamic convoltuional layer are fixed, therefore,\n",
        "# the activations of the dynamic convolutional layer will be pre-calculated \n",
        "# outside the network for efficient implementation.\n",
        "\n",
        "class Net1_DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net1_DCNN, self).__init__()\n",
        "        num_units_fc = 100\n",
        "\n",
        "        self.max_pool = nn.MaxPool1d(2)\n",
        "        self.fc = nn.Linear(int(CONV_FILTERS*(NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2), num_units_fc)\n",
        "        self.out = nn.Linear(num_units_fc, NUM_CLASSES) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, CONV_FILTERS, NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)\n",
        "        x = self.max_pool(x)\n",
        "        x = x.view(-1,int(CONV_FILTERS*(NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2))\n",
        "        x = torch.relu(self.fc(x))\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Net2_DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2_DCNN, self).__init__()\n",
        "        num_units_fc = 100\n",
        "        self.num_inputs_fc = int(CONV_FILTERS2*((NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)/2-CONV_FILTER_SIZE+1)/2)\n",
        "\n",
        "        self.max_pool = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels = CONV_FILTERS, out_channels = CONV_FILTERS2, \n",
        "                               kernel_size=CONV_FILTER_SIZE, padding = 0, stride = 1)\n",
        "        self.max_pool2 = nn.MaxPool1d(2)\n",
        "        self.fc = nn.Linear(self.num_inputs_fc, num_units_fc)\n",
        "        self.out = nn.Linear(num_units_fc, NUM_CLASSES) \n",
        " \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, CONV_FILTERS, NUM_INPUT_FEATURES-CONV_FILTER_SIZE+1)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.max_pool2(x)\n",
        "        x = x.view(-1, self.num_inputs_fc)\n",
        "        x = torch.relu(self.fc(x))\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rrD5WMYjjYr"
      },
      "source": [
        "**5. Function used to evaluate the network**\n",
        "\n",
        "Note: if possible, we run the evaluation on the GPU, this is why a device has to be provided as input argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnItUyMb0y7p"
      },
      "source": [
        "def eval_net(net, test_data, device):\n",
        "  test_dataset = torch.utils.data.TensorDataset( \n",
        "    torch.Tensor(test_data), \n",
        "    torch.LongTensor(test_labels)\n",
        "  )\n",
        "  testloader = torch.utils.data.DataLoader(test_dataset)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, targets in testloader:\n",
        "      inputs = inputs.to(device) \n",
        "      targets = targets.to(device) \n",
        "      outputs = net(inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "      total += targets.size(0)\n",
        "      correct += (predicted == targets)\n",
        "  \n",
        "  return float(correct/total), float(correct), float(total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWeQohOEj_uw"
      },
      "source": [
        "**6. Main experiment loop**\n",
        "\n",
        "Train both networks in 10-fold crossvalidation and evaluate both of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNDm2orqGcrx"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# If available, we use the GPU to train and evaluate the network\n",
        "# (If using Google Colab, make sure that you select a runtime with GPU in\n",
        "# the menu item \"Edit/notebook settings\" or \"Runtime/Change runtime type\".)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(\"Train on GPU\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"GPU is not available, we will use CPU instead of GPU\")\n",
        "\n",
        "# DTW computations will be executed on the CPU\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "accuracies_cnn = []\n",
        "accuracies_dcnn = []\n",
        "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "fold = 0\n",
        "for train_index, test_index in skf.split(data, labels):\n",
        "  fold = fold + 1\n",
        "\n",
        "  train_data = data[train_index]\n",
        "  train_labels = labels[train_index]\n",
        "  test_data = data[test_index]\n",
        "  test_labels = labels[test_index]\n",
        "\n",
        "  # Training of CNN. This is simultaneously the pre-train phase of DCNN.\n",
        "\n",
        "  train_dataset = torch.utils.data.TensorDataset(\n",
        "      torch.Tensor(train_data), \n",
        "      torch.LongTensor(train_labels) \n",
        "  )\n",
        "  trainloader = torch.utils.data.DataLoader(\n",
        "      train_dataset, shuffle=True, batch_size=16)\n",
        "\n",
        "  cnn = Net2_CNN()\n",
        "\n",
        "  #use:\n",
        "  #cnn = Net1_CNN() for experiment with Net1 from the manuscript\n",
        "  #cnn = Net2_CNN() for experiment with Net2 from the manuscript\n",
        "  \n",
        "  cnn.to(device)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(cnn.parameters(), lr=1e-5)\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_n = 0\n",
        "\n",
        "  print(\"Training CNN...\")\n",
        "\n",
        "  for epoch in range(1000):  \n",
        "    for inputs, targets in trainloader:\n",
        "      inputs = inputs.to(device) \n",
        "      targets = targets.to(device) \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      outputs = cnn(inputs)\n",
        "\n",
        "      loss = criterion(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      running_n = running_n + 1\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "      print(\"epoch: {:3d} loss: {:4.3f}\".format(epoch, running_loss/running_n))\n",
        "      running_loss = 0.0\n",
        "      running_n = 0\n",
        "\n",
        "  # Obtain the parameters of the dynamic convolutional layer, and \n",
        "  # precalculate its activations\n",
        "  params = []\n",
        "  for p in cnn.parameters():\n",
        "    params.append(p)\n",
        "\n",
        "  convolutional_filters = np.array(params[0].to(cpu_device).detach().numpy(), \n",
        "                                   dtype=np.float)\n",
        "\n",
        "  dc_activations_train = dc_activations(train_data, convolutional_filters)\n",
        "\n",
        "  # Train DCNN\n",
        "\n",
        "  train_dataset = torch.utils.data.TensorDataset(\n",
        "      torch.Tensor(dc_activations_train), \n",
        "      torch.LongTensor(train_labels) \n",
        "  )\n",
        "  trainloader = torch.utils.data.DataLoader(\n",
        "      train_dataset, shuffle=True, batch_size=16)\n",
        "\n",
        "\n",
        "  #use:  \n",
        "  #dcnn = Net1_DCNN() for experiments with Net1 from the manuscript\n",
        "  #dcnn = Net2_DCNN() for experiments with Net2 from the manuscript\n",
        "\n",
        "  dcnn = Net2_DCNN()\n",
        "\n",
        "  dcnn.to(device)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(dcnn.parameters(), lr=1e-5)\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_n = 0\n",
        "\n",
        "  print(\"Training DCNN...\")\n",
        "\n",
        "  for epoch in range(1000):  \n",
        "    for inputs, targets in trainloader:\n",
        "      inputs = inputs.to(device) \n",
        "      targets = targets.to(device) \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      outputs = dcnn(inputs)\n",
        "\n",
        "      loss = criterion(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      running_n = running_n + 1\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "      print(\"epoch: {:3d} loss: {:4.3f}\".format(epoch, running_loss/running_n))\n",
        "      running_loss = 0.0\n",
        "      running_n = 0\n",
        "  \n",
        "  acc, _, _ = eval_net(cnn, test_data, device)\n",
        "  accuracies_cnn.append(acc)\n",
        "  print(\"Fold: {:2d}, accuracy of CNN:  {:4.3f}\".format(fold, acc))\n",
        "\n",
        "  dc_activations_test = dc_activations(test_data, convolutional_filters)\n",
        "  acc, _, _ = eval_net(dcnn, dc_activations_test, device)\n",
        "  print(\"Fold: {:2d}, accuracy of DCNN: {:4.3f}\".format(fold, acc))\n",
        "  accuracies_dcnn.append(acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Saquk7R6nbSo"
      },
      "source": [
        "Check if the difference is statistically significant and print the results.\n",
        "\n",
        "Please note: due to the random initialisation of the neural networks, if you run the code several times, obtained results may be slightly different (and slightly different from the ones reported in the manuscript)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB-hA5X1rTTW"
      },
      "source": [
        "print(\"**{}**\\n\\n\".format(file_name_prefix.split('/')[-1]))\n",
        "print(\"\\t\\tp-value:        {:4.3f}\".format(scipy.stats.ttest_rel(accuracies_cnn, accuracies_dcnn)[1]))\n",
        "print(\"\\t\\tMean acc. CNN:  {:4.3f}\".format(np.mean(accuracies_cnn)))\n",
        "print(\"\\t\\tMean acc. DCNN: {:4.3f}\".format(np.mean(accuracies_dcnn)))\n",
        "print(\"\\t\\tStd. acc. CNN:  {:4.3f}\".format(np.std(accuracies_cnn)))\n",
        "print(\"\\t\\tStd. acc. DCNN: {:4.3f}\".format(np.std(accuracies_dcnn)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU2fb8TDokjH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}